{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latin-fiber",
   "metadata": {},
   "source": [
    "# A.I. Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-ferry",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Perform some data preproscessing like: data scaling, normalisatin, encoding categorical features\n",
    "* Feel comfortable with simple linear regression\n",
    "* Feel comfortable with a regularization in ML\n",
    "\n",
    "\n",
    "### Content:\n",
    "\n",
    "The Lab. has 3 sections: \n",
    "\n",
    "1. Preprocessing\n",
    "2. Simple Linear regression\n",
    "3. Regularization\n",
    "\n",
    "At the end of each section there is an exercise, each worthing 3 points. All the work must be done during the lab and uploaded on teams by the end of the lab. \n",
    "\n",
    "\n",
    "If there are any python libraries missing, please install them on your working environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "independent-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-auditor",
   "metadata": {},
   "source": [
    "# Section 1. Preprocessing data\n",
    "\n",
    "### Standardization, or mean removal and variance scaling\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fabulous-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91e5e51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                 [ 2.,  0.,  0.],\n",
    "                 [ 0.,  1., -1.]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "incredible-tokyo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "heavy-stereo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sized-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-compact",
   "metadata": {},
   "source": [
    "Scaled data has zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "african-citizen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0. 0. 0.] , std: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean:\", X_scaled.mean(axis=0),\", std:\",  X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-genealogy",
   "metadata": {},
   "source": [
    "It is possible to disable either centering or scaling by either passing $with\\_mean=False$ or $with\\_std=False$ to the constructor of StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-lightweight",
   "metadata": {},
   "source": [
    "### Scaling features to a range\n",
    "\n",
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using *MinMaxScaler* or *MaxAbsScaler*, respectively.\n",
    "\n",
    "Here is an example to scale a simle data matrix to the $[0, 1]$ range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cooperative-confusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "...                     [ 2.,  0.,  0.],\n",
    "...                     [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-deviation",
   "metadata": {},
   "source": [
    "The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "imposed-brother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-package",
   "metadata": {},
   "source": [
    "It is possible to inspect the scaler attributes to find about the exact nature of the transformation learned on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "embedded-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "backed-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-shepherd",
   "metadata": {},
   "source": [
    "If *MinMaxScaler* is given an explicit $feature\\_range=(min, max)$ the full formula is:\n",
    "\n",
    "$$ X_{std} = \\frac{(X - X.min)}{ (X.max - X.min)} $$\n",
    "\n",
    "$$ X_{scaled} = X_{std} * (max - min) + min$$\n",
    "\n",
    "*MaxAbsScaler* works in a very similar fashion, but scales in a way that the training data lies within the range $[-1, 1]$ by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data.\n",
    "\n",
    "Here is how to use the data from the previous example with this scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acknowledged-couple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "...                     [ 2.,  0.,  0.],\n",
    "...                     [ 0.,  1., -1.]])\n",
    "\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "spiritual-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "progressive-miller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-nomination",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\n",
    "\n",
    "The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the $l1$, $l2$, or $max$ norms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "obvious-buyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "...  [ 2.,  0.,  0.],\n",
    "...  [ 0.,  1., -1.]]\n",
    "\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-promise",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "Often features are not given as continuous values but categorical. For example a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as $[0, 1, 3]$ while [\"female\", \"from Asia\", \"uses Chrome\"] would be $[1, 2, 1]$.\n",
    "\n",
    "To convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers ($0$ to $n_{categories} - 1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "closing-miami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrdinalEncoder()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "standard-crossing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-editing",
   "metadata": {},
   "source": [
    "Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).\n",
    "\n",
    "By default, *OrdinalEncoder* will also passthrough missing values that are indicated by *np.nan*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "balanced-attention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-glance",
   "metadata": {},
   "source": [
    "OrdinalEncoder provides a parameter encoded_missing_value to encode the missing values without the need to create a pipeline and using SimpleImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pleased-flour",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'encoded_missing_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5m/hdf175716038jlvhz_ylfzj40000gp/T/ipykernel_2226/2078626025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_missing_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'encoded_missing_value'"
     ]
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-hurricane",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Load the dataset *WA_Fn-UseC_-Telco-Customer-Churn.csv* provided. Perform transformations on it so it is prepared to build a model (scaling the numerical data and the cathegorical features transformed in numerical integer labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# load the dataset\n",
    "dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# replace missing values in 'TotalCharges' with NaN\n",
    "dataset['TotalCharges'] = dataset['TotalCharges'].replace(' ', np.nan)\n",
    "\n",
    "# convert 'TotalCharges' to float type\n",
    "dataset['TotalCharges'] = dataset['TotalCharges'].astype(float)\n",
    "\n",
    "# fill missing values in 'TotalCharges' with the mean of non-missing values\n",
    "dataset['TotalCharges'].fillna(dataset['TotalCharges'].mean(), inplace=True)\n",
    "\n",
    "# transform categorical features into numerical labels\n",
    "categorical_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', \n",
    "            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n",
    "            'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
    "\n",
    "for category in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    dataset[category] = le.fit_transform(dataset[category])\n",
    "\n",
    "# scale the numerical data\n",
    "num_data = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dataset[num_data] = scaler.fit_transform(dataset[num_data])\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-prior",
   "metadata": {},
   "source": [
    "# Section 2. Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-content",
   "metadata": {},
   "source": [
    "Linear regression is defined as an algorithm that provides a linear relationship between an independent variable and a dependent variable to predict the outcome of future events. \n",
    "\n",
    "Most of the major concepts in machine learning can be and often are discussed in terms of various linear regression models. Thus, this section will introduce you to building and fitting linear regression models and some of the process behind it, so that you can \n",
    "\n",
    "1. fit models to data you encounter \n",
    "\n",
    "1. experiment with different kinds of linear regression and observe their effects\n",
    "\n",
    "1. see some of the technology that makes regression models work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-shanghai",
   "metadata": {},
   "source": [
    "### Linear regression with a simple dataset\n",
    "\n",
    "Lets build first a very problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i \\in \\{ 1, 2, 3 \\}$,\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $a x + b = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the *least-squares sense*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1,2,3])\n",
    "y_train = np.array([2,3,6])\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(3,1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a simple scatterplot\n",
    "plt.scatter(x_train,y_train)\n",
    "\n",
    "# check dimensions \n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_scatterplot(x, y, title):\n",
    "    # font size\n",
    "    f_size = 18\n",
    "    \n",
    "    # make the figure\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8,5)) # Create figure object\n",
    "\n",
    "    # set axes limits to make the scale nice\n",
    "    ax.set_xlim(np.min(x)-1, np.max(x) + 1)\n",
    "    ax.set_ylim(np.min(y)-1, np.max(y) + 1)\n",
    "\n",
    "    # adjust size of tickmarks in axes\n",
    "    ax.tick_params(labelsize = f_size)\n",
    "    \n",
    "    # remove tick labels\n",
    "    ax.tick_params(labelbottom=False,  bottom=False)\n",
    "    \n",
    "    # adjust size of axis label\n",
    "    ax.set_xlabel(r'$x$', fontsize = f_size)\n",
    "    ax.set_ylabel(r'$y$', fontsize = f_size)\n",
    "    \n",
    "    # set figure title label\n",
    "    ax.set_title(title, fontsize = f_size)\n",
    "\n",
    "    # you may set up grid with this \n",
    "    ax.grid(True, lw=1.75, ls='--', alpha=0.15)\n",
    "\n",
    "    # make actual plot (Notice the label argument!)\n",
    "    #ax.scatter(x, y, label=r'$My points$')\n",
    "    #ax.scatter(x, y, label='$My points$')\n",
    "    ax.scatter(x, y, label=r'$my\\,points$')\n",
    "    ax.legend(loc='best', fontsize = f_size);\n",
    "    \n",
    "    return ax\n",
    "\n",
    "nice_scatterplot(x_train, y_train, 'A nice plot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-nickname",
   "metadata": {},
   "source": [
    "#### Formulae\n",
    "Linear regression is special among the models we study because it can be solved explicitly. While most other models (and even some advanced versions of linear regression) must be solved itteratively, linear regression has a formula where you can simply plug in the data.\n",
    "\n",
    "For the single predictor case it is:\n",
    "    \\begin{align}\n",
    "      a &= \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}\\\\\n",
    "      b &= \\bar{y} - a \\bar{x}\\\n",
    "    \\end{align}\n",
    "    \n",
    "Where $\\bar{y}$ and $\\bar{x}$ are the mean of the y values and the mean of the x values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-vegetation",
   "metadata": {},
   "source": [
    "###  Building a model from scratch\n",
    "\n",
    "We will solve the equations for simple linear regression and find the best fit solution to our simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-disability",
   "metadata": {},
   "source": [
    "The snippets of code below implement the linear regression equations on the observed predictors and responses, which we'll call the training data set.  Let's walk through the code.\n",
    "\n",
    "We have to reshape our arrrays to 2D. We will see later why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "xx = np.array([[1,2,3],[4,6,8]])\n",
    "xxx = xx.reshape(-1,2)\n",
    "xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to be a proper 2D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, compute means\n",
    "y_bar = np.mean(y_train)\n",
    "x_bar = np.mean(x_train)\n",
    "\n",
    "# build the two terms\n",
    "numerator = np.sum( (x_train - x_bar)*(y_train - y_bar) )\n",
    "denominator = np.sum((x_train - x_bar)**2)\n",
    "\n",
    "print(numerator.shape, denominator.shape) #check shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-overall",
   "metadata": {},
   "source": [
    "* Why the empty brackets? (The numerator and denominator are scalars, as expected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope beta1\n",
    "a = numerator/denominator\n",
    "\n",
    "#intercept beta0\n",
    "b = y_bar - a * x_bar\n",
    "\n",
    "print(\"The best-fit line is {0:3.2f} + {1:3.2f} * x\".format(b, a))\n",
    "print(f'The best fit is {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression_fit(x_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x_train: a (num observations by 1) array holding the values of the predictor variable\n",
    "    y_train: a (num observations by 1) array holding the values of the response variable\n",
    "\n",
    "    Returns:\n",
    "    beta_vals:  a (num_features by 1) array holding the intercept and slope coeficients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check input array sizes\n",
    "    if len(x_train.shape) < 2:\n",
    "        print(\"Reshaping features array.\")\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "\n",
    "    if len(y_train.shape) < 2:\n",
    "        print(\"Reshaping observations array.\")\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "    # first, compute means\n",
    "    y_bar = np.mean(y_train)\n",
    "    x_bar = np.mean(x_train)\n",
    "\n",
    "    # build the two terms\n",
    "    numerator = np.sum( (x_train - x_bar)*(y_train - y_bar) )\n",
    "    denominator = np.sum((x_train - x_bar)**2)\n",
    "    \n",
    "    #slope a\n",
    "    a = numerator/denominator\n",
    "\n",
    "    #intercept b\n",
    "    b = y_bar - beta_1*x_bar\n",
    "\n",
    "    return np.array([b,a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-ridge",
   "metadata": {},
   "source": [
    "* Let's run this function and see the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1 ,2, 3])\n",
    "y_train = np.array([2, 2, 4])\n",
    "\n",
    "coeficients = simple_linear_regression_fit(x_train, y_train)\n",
    "\n",
    "a = coeficients[1]\n",
    "b = coeficients[0]\n",
    "\n",
    "print(\"The best-fit line is {1:8.6f} * x + {0:8.6f}.\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/best_fit_scatterplot.py\n",
    "fig_scat, ax_scat = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "# Plot best-fit line\n",
    "x_train = np.array([[1, 2, 3]]).T\n",
    "\n",
    "best_fit = b + a * x_train\n",
    "\n",
    "ax_scat.scatter(x_train, y_train, s=300, label='Training Data')\n",
    "ax_scat.plot(x_train, best_fit, ls='--', label='Best Fit Line')\n",
    "\n",
    "ax_scat.set_xlabel(r'$x_{train}$')\n",
    "ax_scat.set_ylabel(r'$y$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-brisbane",
   "metadata": {},
   "source": [
    "The values of `a` and `b` seem roughly reasonable.  They capture the positive correlation.  The line does appear to be trying to get as close as possible to all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-bullet",
   "metadata": {},
   "source": [
    "## 4 - Building a model with `statsmodels` and `sklearn`\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two `python` packages to do it all for us:\n",
    "* [statsmodels](http://www.statsmodels.org/stable/regression.html) and \n",
    "* [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $a$ and $b$ from `statsmodels` and `sklearn` to the ones that we found from above with our own implementation.\n",
    "\n",
    "For the purposes of this lab, `statsmodels` and `sklearn` do the same thing.  More generally though, `statsmodels` tends to be easier for inference \\[finding the values of the slope and intercept and dicussing uncertainty in those values\\], whereas `sklearn` has machine-learning algorithms and is better for prediction \\[guessing y values for a given x value\\]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for.\n",
    "\n",
    "**Note:** `statsmodels` and `sklearn` are different packages!  Unless we specify otherwise, you can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-lemon",
   "metadata": {},
   "source": [
    "below is the code for `statsmodels`.  `Statsmodels` does not by default include the column of ones in the $X$ matrix, so we include it manually with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_train)\n",
    "\n",
    "# this is the same matrix as in our scratch problem!\n",
    "print(X)\n",
    "\n",
    "# build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "\n",
    "# do the fit and save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(f'The regression coef from statsmodels are: beta_0 = {beta0_sm:8.6f} and beta_1 = {beta1_sm:8.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-kruger",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(results_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-warrior",
   "metadata": {},
   "source": [
    "Now let's turn our attention to the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the least squares model\n",
    "toyregr = linear_model.LinearRegression()\n",
    "\n",
    "# save regression info (parameters, etc) in results_skl\n",
    "results = toyregr.fit(x_train, y_train)\n",
    "\n",
    "# pull the beta parameters out from results_skl\n",
    "beta0_skl = toyregr.intercept_\n",
    "beta1_skl = toyregr.coef_[0]\n",
    "\n",
    "print(\"The regression coefficients from the sklearn package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f}\".format(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-dining",
   "metadata": {},
   "source": [
    "Same results! We can try a real problem now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-charles",
   "metadata": {},
   "source": [
    "### The `scikit-learn` library and the shape of things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-allen",
   "metadata": {},
   "source": [
    "Before diving into a \"real\" problem, let's discuss more of the details of `sklearn`.\n",
    "\n",
    "`Scikit-learn` is the main `Python` machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split()`. \n",
    "\n",
    "Use the following to add the library into your code:\n",
    "\n",
    "```python\n",
    "import sklearn \n",
    "```\n",
    "\n",
    "In `scikit-learn`, an **estimator** is a Python object that implements the methods `fit(X, y)` and `predict(T)`\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `fit()` always takes two arguments:\n",
    "```python\n",
    "estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider one estimator in this lab: `LinearRegression`.\n",
    "\n",
    "It is very important to understand that `Xtrain` must be in the form of a **2x2 array** with each row corresponding to one sample, and each column corresponding to the feature values for that sample.\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the dataset (be sure that this file is in the same folder with the j. notebook)\n",
    "df = pd.read_csv('Salary_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.head())\n",
    "X = np.array(df['YearsExperience'])\n",
    "y = np.array(df['Salary'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rember to reshape the X list in order to have a two dimensional array. \n",
    "# Since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# we perform the regression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Linear Regression-Training set score: {lr.score(X_train, y_train):.2f}\")\n",
    "print(f\"Linear Regression-Test set score: {lr.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-tuning",
   "metadata": {},
   "source": [
    "To fnd the coeficients from the formula $ax + b = y$ we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lr.coef_[0] # we ahve one feature with index 0\n",
    "\n",
    "b = lr.intercept_ # a scalar\n",
    " \n",
    "print(a, \"* x +\", b, \"= y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-powder",
   "metadata": {},
   "source": [
    "Now that we have the model let's make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-passport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.predict([[20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "a * 20 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-track",
   "metadata": {},
   "source": [
    "In other words our model predicted that we get after 20 years a salary of 213643.9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-chess",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = lr.predict([[20]])\n",
    "fig_scat, ax_scat = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "\n",
    "x = X.T\n",
    "\n",
    "best_fit = b + a * (np.append(x, [20]))\n",
    "\n",
    "ax_scat.scatter(x, y, s=300, label='Training Data')\n",
    "ax_scat.plot(np.append(x,[20]), best_fit, ls='--', label='Best Fit Line')\n",
    "\n",
    "ax_scat.plot([20],pred, \"ys\", label=\"LinearRegression\")\n",
    "ax_scat.set_xlabel(r'$x$')\n",
    "ax_scat.set_ylabel(r'$y$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-spray",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Download from https://www.kaggle.com/ the regression dataset: Student Study Hours. Create a model and make 3 predictions. Make some nice graphics to depict the model (training set, test set, predictions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-python",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"student_hours_vs_marks.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X = np.array(df['Hours'])\n",
    "y = np.array(df['Scores'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# reshape the X list in order to have a two dimensional array\n",
    "# since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Linear Regression-Training set score: {model.score(X_train, y_train):.2f}\")\n",
    "print(f\"Linear Regression-Test set score: {model.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# make three predictions\n",
    "X_new = [[2.5], [5.0], [10.0]]\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# plot the training set\n",
    "plt.scatter(X_train, y_train, s=300)\n",
    "plt.plot(X_train, model.predict(X_train), color=\"red\")\n",
    "plt.title(\"Training set\")\n",
    "plt.xlabel(\"Study hours\")\n",
    "plt.ylabel(\"Marks\")\n",
    "plt.show()\n",
    "\n",
    "# plot the testing set\n",
    "plt.scatter(X_test, y_test, s=300)\n",
    "plt.plot(X_train, model.predict(X_train), color=\"red\")\n",
    "plt.title(\"Testing set\")\n",
    "plt.xlabel(\"Study hours\")\n",
    "plt.ylabel(\"Marks\")\n",
    "plt.show()\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(X_new, y_pred, s=300)\n",
    "plt.plot(X_train, model.predict(X_train), color=\"red\")\n",
    "plt.title(\"Predictions\")\n",
    "plt.xlabel(\"Study hours\")\n",
    "plt.ylabel(\"Marks\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pred = model.predict([[10]])\n",
    "a = model.coef_[0] # we have one feature with index 0\n",
    "b = model.intercept_ # a scalar\n",
    "print(a, \"* x +\", b, \"= y\")\n",
    "x = X.T\n",
    "best_fit = b + a * (np.append(x, [10]))\n",
    "plt.scatter(x, y, s=300)\n",
    "plt.plot(np.append(x,[10]), best_fit, color=\"red\")\n",
    "plt.plot([10], pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-breast",
   "metadata": {},
   "source": [
    "# Section 3. Regularization\n",
    "\n",
    "- restricting a model to avoid overfitting by shrinking the coefficient estimates to zero. \n",
    "\n",
    "To avoid overfitting we control the model’s complexity by adding a penalty to the model’s loss function:\n",
    "\n",
    "$$\\text{ Regularization} = \\text{Loss Function} + \\text{Penalty} $$\n",
    "\n",
    "There are three commonly used regularization techniques to control the complexity of machine learning models, as follows:\n",
    "\n",
    "* L2 regularization\n",
    "* L1 regularization\n",
    "* Elastic Net\n",
    "\n",
    "\n",
    "## L2 regularisation\n",
    "\n",
    "A *ridge* regression -- a regularization term is added to the cost function of the linear regression, which keeps the magnitude of the model’s weights (coefficients) as small as possible. The L2 regularization technique tries to keep the model’s weights close to zero, but not zero, which means each feature should have a low impact on the output while the model’s accuracy should be as high as possible.\n",
    " \n",
    " $$ \\text{Ridge Regression Cost Function} = \\text{Loss Function} + \\frac{1}{2}\\lambda \\sum_{j=1}^m \\omega_j^2$$\n",
    "\n",
    "Where $\\lambda$  controls the strength of regularization, and  $\\omega$ are the model’s weights (coefficients).\n",
    "\n",
    "By increasing $\\lambda$, the model becomes flattered and underfit. On the other hand, by decreasing $\\lambda$, the model becomes more overfit, and with $\\lambda = 0$, the regularization term will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rember to reshape the X list in order to have a two dimensional array. \n",
    "# Since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.7).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Ridge Regression-Training set score: {ridge.score(X_train, y_train):.2f}\")\n",
    "print(f\"Ridge Regression-Test set score: {ridge.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-copper",
   "metadata": {},
   "source": [
    "## L1 Regularization\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (lasso) regression is an alternative to ridge for regularizing linear regression. Lasso regression also adds a penalty term to the cost function, but slightly different, called $L1$ regularization. $L1$ regularization makes some coefficients zero, meaning the model will ignore those features. Ignoring the least important features helps emphasize the model’s essential features.\n",
    "\n",
    "$$ \\text{Lasso Regrestion Cost Function} = \\text{Loss Function} + r \\lambda \\sum_{j=1}^m |wj|$$\n",
    "\n",
    "Where $\\lambda$  controls the strength of regularization, and $\\omega$ are the model’s weights (coefficients).\n",
    "\n",
    "Lasso regression automatically performs feature selection by eliminating the least important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rember to reshape the X list in order to have a two dimensional array. \n",
    "# Since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=1.0).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Lasso Regression-Training set score: {lasso.score(X_train, y_train):.2f}\")\n",
    "print(f\"Lasso Regression-Test set score: {lasso.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-sauce",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "The Elastic Net is a regularized regression technique combining ridge and lasso’s regularization terms. The \n",
    " parameter controls the combination ratio. When \n",
    ", the L2 term will be eliminated, and when \n",
    ", the L1 term will be removed.\n",
    "\n",
    "$$\\text{Elastic Net Cost Function} = \\text{Loss Function} + r \\lambda \\sum_{j=1}^m |wj|+ \\dfrac{(1-r)}{2} \\lambda\\sum_{j=1}^m w_j^2$$\n",
    "\n",
    "Although combining the penalties of lasso and ridge usually works better than only using one of the regularization techniques, adjusting two parameters, \n",
    " and \n",
    ", is a little tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rember to reshape the X list in order to have a two dimensional array. \n",
    "# Since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.01).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Elastic Net-Training set score: {elastic_net.score(X_train, y_train):.2f}\")\n",
    "print(f\"Elastic Net-Test set score: {elastic_net.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-studio",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "For your build previous model perform all three regularizations presented here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"student_hours_vs_marks.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X = np.array(df['Hours'])\n",
    "y = np.array(df['Scores'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# reshape the X list in order to have a two dimensional array\n",
    "# since we have only one feature the reshape looks like below: \n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Linear Regression-Training set score: {model.score(X_train, y_train):.2f}\")\n",
    "print(f\"Linear Regression-Test set score: {model.score(X_test, y_test):.2f}\")\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=0.7).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Ridge Regression-Training set score: {ridge.score(X_train, y_train):.2f}\")\n",
    "print(f\"Ridge Regression-Test set score: {ridge.score(X_test, y_test):.2f}\")\n",
    "\n",
    "\n",
    "lasso = Lasso(alpha=1.0).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Lasso Regression-Training set score: {lasso.score(X_train, y_train):.2f}\")\n",
    "print(f\"Lasso Regression-Test set score: {lasso.score(X_test, y_test):.2f}\")\n",
    "\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.01).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Elastic Net-Training set score: {elastic_net.score(X_train, y_train):.2f}\")\n",
    "print(f\"Elastic Net-Test set score: {elastic_net.score(X_test, y_test):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

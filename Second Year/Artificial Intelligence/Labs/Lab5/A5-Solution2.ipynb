{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb25091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (hidden): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (ReLU): ReLU()\n",
      "  (output): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "Model 1 reached 100% accuracy at epoch 65\n",
      "tensor([[0.4996, 0.5623],\n",
      "        [0.4996, 0.5623],\n",
      "        [0.5256, 0.5662],\n",
      "        [0.4996, 0.5623]], grad_fn=<SigmoidBackward0>)\n",
      "Model Accuracy: 1.0%\n",
      "Model weights:\n",
      "hidden.weight tensor([[-0.0044, -0.4770],\n",
      "        [ 0.4587, -0.6992],\n",
      "        [-0.6169, -0.5928]])\n",
      "hidden.bias tensor([-0.6294, -0.1852, -0.2975])\n",
      "output.weight tensor([[ 0.0989,  0.3759, -0.3622],\n",
      "        [ 0.3522,  0.0578, -0.2967]])\n",
      "output.bias tensor([-0.0079,  0.2490])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model1 = nn.Sequential(OrderedDict([\n",
    "    ('hidden', nn.Linear(2, 3)),  # first hidden layer with 3 neurons\n",
    "    ('ReLU', nn.ReLU()),          # relu activation function\n",
    "    ('output', nn.Linear(3, 2)),  # output layer with 3 neurons\n",
    "    ('sigmoid', nn.Sigmoid())     # sigmoid activation function\n",
    "]))\n",
    "print(model1)\n",
    "\n",
    "data_in1 = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float)\n",
    "print(data_in1)\n",
    "\n",
    "data_target1 = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float)\n",
    "print(data_target1)\n",
    "\n",
    "criterion1 = nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.1)\n",
    "\n",
    "# train the model\n",
    "losses1 = []\n",
    "for epoch in range(1000):\n",
    "    # reset the gradients to zero before each forward and backward pass\n",
    "    optimizer1.zero_grad()    \n",
    "    outputs1 = model1(data_in1) # forward pass\n",
    "    loss1 = criterion1(outputs1, data_target1) # calculate loss\n",
    "    loss1.backward()          # backward pass\n",
    "    optimizer1.step()         # update weights\n",
    "    losses1.append(loss1.item())\n",
    "    predicted_classes1 = (outputs1.round() == data_target1)\n",
    "    accuracy1 = predicted_classes1.sum().item() / len(data_target1)\n",
    "    if accuracy1 == 1:\n",
    "        print(f\"Model 1 reached 100% accuracy at epoch {epoch+1}\")\n",
    "        break\n",
    "        \n",
    "# visualize the results\n",
    "print(outputs1)\n",
    "predicted = torch.round(outputs1)\n",
    "correct = (predicted == data_target1).sum()\n",
    "total = data_target1.size(0)\n",
    "accuracy = correct / total\n",
    "print(f'Model Accuracy: {accuracy}%') \n",
    "\n",
    "# print model wights\n",
    "print(f\"Model weights:\")\n",
    "for name, param in model1.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b985dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
